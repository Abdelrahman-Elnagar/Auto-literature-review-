filename,extraction_status,Paper Title,Authors,Year of Publication,Meta-Learning Method Used,Meta-Features Chosen,Algorithm Selection Method,Algorithms Considered for Selection,Evaluation Metrics,Dataset(s) Used,Performance of Meta-Learning Approach,Key Findings/Contributions,Meta-Feature Generation Process,Limitations,Simple Summary,IEEE Citation
2205.10362v2.pdf,success,FIND: Explainable Framework for Meta-learning,"Xinyue Shao, Hongzhi Wang, Xiao Zhu, and Feng Xiong",2018,"Neural network with one input layer, two hidden layers, and one output layer","23 meta-features are used, including target attribute categories, number of category attributes, number of numeral attributes, information entropy of attribute values within each category, etc.; Three types: simple (size, number of attributes, number of categories), statistical (geometric mean, variance), and information-theoretic (information entropy)","A meta-learning approach is used to recommend algorithms.  Details of the specific algorithm used for selection (e.g., k-NN, etc.) are not provided, but it involves a neural network with a dropout rate of 0.5 and a learning rate of 0.001.; Classification algorithms (mentioned as having a wide selection) and potentially regression algorithms (referenced from previous work); Meta-learning approach using a neural network trained on dataset meta-features and best-performing algorithms. Integrated gradients are used to explain the meta-learning model's recommendations.; Model recommendation","Not explicitly mentioned in this chunk, other than S1, S2,...Sm; adaboostM1, Random Forest, MLP (Multi-layer Perceptron), ClassificationViaRegression, XGBoost","Accuracy, Precision, Recall, F-measure, F1, F2, F3 (comparison metrics for recommended vs. labeled methods); Reliability and efficiency of counterfactual generation, average distance of counterfactuals from original instances, average time cost for counterfactual generation","A custom metadata set created by the authors, including datasets like anneal, D41, D74, heart-statlog, liver-disorders, monks-problems-1, sick.  Adult and German-Credit datasets are used for evaluating feature influence computation.; Adult, German-Credit; Common machine learning datasets (not specifically named)","Achieved over 80% accuracy in recommending algorithms, outperforming Random Forest (61.61%) and XGBoost (75%). The recommended methods also showed comparable performance to the labeled methods even when they were inconsistent.","(1) Proposing FIND, a comprehensive framework for explainable meta-learning based on causal relation. (2) Developing an interpretable learning algorithm recommendation approach based on dataset meta-features. (3) Suggesting a novel feature importance metric based on causality and a greedy counterfactual generation approach. (4) Evaluating the framework's accuracy and validity on real datasets.; Explainable meta-learning for algorithm selection using integrated gradients. Proposes a feature influence computation method incorporating causality and latent factors.; Proposed and implemented FIND meta-learning explainability framework which uses model recommendation to achieve meta-explainability. Recommendation explainability is achieved by feature influence computation and counterfactual generation.; The proposed meta-learning module can effectively recommend efficient decision methods. The integrated gradients analysis provides insights into the relationship between dataset characteristics and suitable algorithms. The counterfactual generation algorithm considers causal relationships and feature value constraints for more plausible counterfactuals.","Collected data on target attribute categories, number of category/numeral attributes, information entropy of attribute values, and other factors.; Extracted from datasets using statistical algorithms. Includes simple, statistical, and information-theoretic features.","Explainability in more challenging meta-learning domains (e.g., time series) and deeper counterfactual exploration incorporating causality need further investigation.; Not explicitly mentioned in this chunk","This paper introduces FIND, a framework for explainable meta-learning that recommends machine learning algorithms based on 23 dataset meta-features using a neural network.  FIND achieved over 80% accuracy in algorithm recommendation, outperforming baseline methods like Random Forest and XGBoost, and provides explanations for its recommendations through integrated gradients and a novel causality-based feature importance metric.  The framework also offers a counterfactual generation approach to explore alternative algorithm choices by suggesting changes to dataset characteristics.","Xinyue Shao, Hongzhi Wang, Xiao Zhu, et al., ""FIND: Explainable Framework for Meta-learning,"" in Meta-Learning Research, 2018."
2410.07696v1.pdf,success,Meta-Learning from Learning Curves for Budget-Limited Algorithm Selection,"Manh Hung Nguyen, Lisheng Sun-Hosoya, and Isabelle Guyon",2024,"Reinforcement learning (specifically, Double Deep Q-Network (DDQN)) was used as a baseline method, and the winning teams also employed meta-learning, although the specific methods they used are not detailed.","Dataset meta-features and algorithm hyperparameters were provided, but specific features chosen by participants are not mentioned.; Dataset meta-features and learning curve progression were frequently used. Algorithm hyperparameters were less commonly used.","Reinforcement Learning (specifically, agents were submitted to a challenge); Reinforcement learning (specifically, a Markov Decision Process framed as a REVEAL game) where an agent learns a policy to select algorithms based on partial learning curves, dataset meta-features, and algorithm hyperparameters.; Several methods were used, including learned policies (e.g., DDQN), combined learned and hard-coded policies (winning teams), average ranking (AvgRank), best performance within a small budget (BoS), Bayesian Optimization (Freeze-Thaw), and random search (RandSearch).","A set of 40 algorithms was used, generated by varying the hyperparameters of several base algorithms from the AutoML challenge.; Round 1: Modified AutoML baseline with varying hyperparameters, using tree-based algorithms (Random Forest, Gradient Boosting) as the core. Round 2: Nearest Neighbors, Multilayer Perceptron, Adaboost, and Stochastic Gradient Descent.; SGD, AdaBoost, KNN","Area under the learning curve (ALC) of the agent's performance on test datasets.; Average accumulated reward over meta-test datasets, Area under the Learning Curve (ALC).; Performance on datasets (e.g., Flora, waldo, pablo, etc.) and comparison to baseline methods like Best on Samples (BoS) and Double Deep Q-Network (DDQN)","30 cross-domain AutoML datasets provided by [9], along with a synthetic meta-dataset created by [17, 18] for practice.; Learning curve datasets (e.g., Flora, waldo, pablo, marco, evita, wallis, jannis, dionis, alexis, cadata, carlo, digits, dorothea, fabert).  A public repository with the datasets is mentioned.; Meta-datasets derived from the AutoML challenge, containing pre-recorded learning curves.  Examples include 'tania', 'robert', 'newsgroups', and 'marco'. The datasets were split into meta-training and meta-testing sets.","Mentioned that meta-learning enhances algorithm selection, but specific performance metrics are not provided in this chunk.; The top-performing teams outperformed the DDQN baseline in some settings.  Specifically, in the first round, the top 3 teams outperformed DDQN in the Any-time learning setting. In the second round, one team outperformed DDQN in Any-time learning and two teams outperformed it in Fixed-time learning.; Winning agents using meta-learned knowledge outperformed those without, particularly demonstrated by their action trajectories. BoS surprisingly outperformed DDQN on some datasets (9/30 in the first round and 5/15 in the second round) where learning curves didn't intersect frequently.","(i) Formulation of the limited-budget algorithm selection problem as an MDP. (ii) Introduction of new benchmark datasets of learning curves. (iii) Comprehensive result analysis comparing data usage and policy types, including an ablation study showing the benefits of meta-learning and learning from learning curves.; Agents leveraging meta-learned knowledge of learning curve progressions perform better. A simple BoS method can be a competitive baseline. The challenge datasets serve as a benchmark for future research.; Learning curve progression and meta-learning were found to be crucial for good performance. Switching explored algorithms too frequently is detrimental. Spending a small budget initially to get a ""base score"" and avoiding small budgets near the end of an episode were beneficial strategies.",Meta-features and hyperparameters were provided as part of the datasets; Not explicitly mentioned.; the generation process is not detailed in this chunk.,The high correlation between validation and test learning curves in the first round posed a risk of overfitting. The interpolation of unrecorded points on the learning curves in the first round did not provide new information but still incurred a cost.,"This paper introduces a novel Markov Decision Process (MDP) framework for budget-limited algorithm selection, using reinforcement learning agents to select algorithms based on partial learning curves, dataset meta-features, and hyperparameters.  Evaluation on a new benchmark of learning curve datasets revealed that meta-learning, particularly by leveraging learning curve progression, significantly improves algorithm selection performance, while even simple baselines like ""Best on Samples"" can be surprisingly competitive.  The study highlights the importance of strategic budget allocation and the detrimental effects of frequent algorithm switching.","Manh Hung Nguyen, Lisheng Sun-Hosoya, and Isabelle Guyon, ""Meta-Learning from Learning Curves for Budget-Limited Algorithm Selection,"" in Meta-Learning Research, 2024."
A_Literature_Survey_and_Empirical_Study_of_Meta-Learning_for_Classifier_Selection.pdf,success,A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection,"Irfan Khan, Xianchao Zhang, Mobashar Rehman, and Rahman Ali",2020,"Several meta-learning methods for algorithm recommendation are compared, including:  ML-KNN (multi-label k-nearest neighbors), k-nearest neighbors, link prediction, and clustering-based approaches.","(i) Statistical and Information-Theory-Based measures (ii) Problem-Complexity-Based measures (iii) Model-Structure-Based measures (iv) Landmarking-Based measures (v) Structural-Information-Based measures; Discusses various groups of meta-features used for classifier selection, including simple, statistical, information-theoretic, problem-complexity-based, model-structure-based, and landmarking-based measures.  Specific examples of measures within these groups are mentioned, such as Maximum Fisher's discriminant ratio and Entropy of Class Proportions.; Five groups of meta-features are used, though they are not explicitly named here.  The text mentions ""statistical, information-theoretic and structural information based"" meta-features.; Statistical, structural, and information-based meta-features, including frequencies of itemsets with respect to the parity function.  Specifically, one-itemsets (VI) capturing attribute value distributions and two-itemsets (VII) representing feature correlations are used. Statistical summaries (min, max, and seven octiles) of these itemsets are computed.","Algorithm selection is based on the meta-learning approaches mentioned above.  Methods predict a set of suitable algorithms based on the meta-features of a new dataset.; Generic framework described involving meta-knowledge construction (performance evaluation and dataset characterization), mapping model construction (transforming meta-data into a learning dataset and training a meta-learner), and recommendation for new problem instances. Multiple Comparison Procedure (MCP), specifically Friedman test followed by Holm procedure, is mentioned for identifying best-performing algorithms.; Several methods (A-E) are compared, including multi-label k-nearest neighbors (ML-KNN), instance-based k-nearest neighbors (KNN), link prediction in a heterogeneous network, clustering-based selection, and regression-based selection.; Various meta-learners are discussed, including decision trees (C4.5, C5.0), k-Nearest Neighbors (KNN), regression algorithms, multi-label learning (ML-KNN), clustering, and link prediction (LRW, SRW).  Selection is based on the learned model's output, which can be a best algorithm, ranked list, or multiple algorithms.","Candidate algorithms (algorithm space A) are selected to potentially solve problem instances. Specific algorithms are not listed in this section.; Classification algorithms (17 mentioned in later sections); Ripper, PART, Bayes Network, Naive Bayes, C4.5, Support Vector Machine, K-Nearest Neighbors, RandomTree, RandomForest, Boosting, and Bagging with IBL, Naive Bayes, PART, and C4.5.; The text mentions 22 algorithms used with C4.5, 8 with C5.0, 10 with KNN in the DMA system, and 13 with ML-KNN. 21 algorithms are mentioned in the context of link prediction.  Specific algorithm names are not provided in most cases.","Accuracy and runtime are mentioned as performance evaluation measures.; Adjusted Ratio of Ratios (ARR) is mentioned as a multi-criteria performance evaluation metric. Other metrics are not explicitly stated in this section.; Hit Rate and Recommendation Accuracy. Hit Rate measures the probability that the recommended set of algorithms contains at least one truly appropriate algorithm. Recommendation Accuracy penalizes the recommendation of inappropriate algorithms.; Performance metrics (e.g., accuracy, runtime - mentioned in later sections); Recommendation Accuracy, Hit Ratio, and Adjusted Ratio of Ratios (ARR)","82 classification datasets from the UCI repository; 84 benchmark datasets (mentioned in later sections); A subset of diverse classification problems (problem space P) are used for building the meta-knowledge database. Specific datasets are not named in this section.; The number of datasets used in different studies varies. Some studies used less than 100, while others used more than 100.  67 datasets are mentioned in connection with the DMA system, 84 with ML-KNN, and 131 with the link prediction approach. Specific dataset names are not provided.","Link prediction, ML-KNN, and KNN showed good performance in terms of Hit Rate (around 90% or higher).  Link prediction also performed well on Recommendation Accuracy (52%). The clustering-based method performed well on Hit Rate but poorly on Recommendation Accuracy. The performance of all methods slightly decreased when considering runtime in addition to accuracy.; Not explicitly mentioned in this section, beyond general statements about advantages and disadvantages of different approaches.; Not explicitly mentioned in this section, but a comparative study is being conducted.; Not explicitly mentioned in this section, but the text discusses the need for a detailed comparative study to quantitatively assess the performance of different methods.","A comparative study is being conducted to address shortcomings of previous research by comparing prominent classifier selection methods, standardizing meta-feature generation, using the same problem and algorithm space for all methods, and assessing performance on each group of meta-features.; First survey and comparative study on meta-learning for classification algorithm selection. Presents a generic workflow for meta-learning based algorithm recommendation. Critically analyzes existing studies based on meta-features, meta-learner, and meta-target.; The study provides a comprehensive comparison of different meta-learning approaches for algorithm recommendation.  It highlights the strengths and weaknesses of each method and identifies areas for future research, such as developing more efficient meta-features and addressing the computational cost of meta-learning.; This paper presents the first literature survey and extensive empirical study specifically focused on meta-learning based classifier selection. It aims to analyze previous research regarding the problem space, datasets, algorithms, meta-features, meta-targets, mapping approaches, and evaluation measures used in meta-learning for classifier selection.; This section summarizes different meta-learning approaches for algorithm selection, highlighting their meta-learners, meta-targets, and limitations. It emphasizes the importance of dataset diversity and representativeness at the meta-level.","A standardized meta-feature generation framework is used [49], [50]; Described for several meta-features, including formulas and explanations of their purpose.  References are provided for further details.; For the structural and information-based meta-features, the process involves converting the dataset to a binary format, generating one-itemsets (VI) and two-itemsets (VII), sorting them, and then calculating statistical summaries.; Not explicitly mentioned, but the text mentions that current methods provide a ""global overview"" and suggests exploring techniques that consider problem distribution.","Existing literature lacks a comprehensive survey and comparative study of meta-learning methods for classifier selection.  Each new method is typically compared with only one or two prior methods.; Previous studies often compare newly proposed methods with only one or two other methods, use the same group of meta-features without necessarily using the same extraction measures, and employ diverse datasets and candidate algorithms, making it difficult to quantitatively compare performance.; Previous studies used varying meta-features, datasets, and algorithms, making comparison difficult.  Earlier studies primarily considered accuracy for performance evaluation.; The paper mentions the high computational cost associated with evaluating candidate algorithms and extracting meta-features.  It also notes the need for more optimal meta-feature extraction techniques and a more detailed analysis of meta-feature combinations to reduce redundancy.; The text discusses limitations of various approaches, such as the difficulty of selecting an optimal K for KNN, the computational cost of retraining regression models, the lack of information about performance differences in ranked lists, and the potential bias introduced by dataset selection at the meta-level.","This 2020 paper by Khan et al. presents the first comprehensive survey and comparative study of meta-learning for classifier selection, evaluating several methods like ML-KNN, k-NN, link prediction, and clustering using a standardized meta-feature generation process across a consistent set of datasets and algorithms.  Their findings indicate that link prediction, ML-KNN, and k-NN achieve high Hit Rates, with link prediction also demonstrating strong Recommendation Accuracy, contributing valuable insights into the effectiveness of different meta-learning approaches for automated algorithm selection.","Irfan Khan, Xianchao Zhang, Mobashar Rehman, et al., ""A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection,"" in Meta-Learning Research, 2020."
cruz2015.pdf,success,META-DES: A dynamic ensemble selection framework using meta-learning,"Rafael M.O. Cruz, Robert Sabourin, George D.C. Cavalcanti, Tsang Ing Ren",2014,A Multi-Layer Perceptron (MLP) neural network is trained using the Levenberg-Marquadt algorithm. The training process stops if performance on the validation set decreases or fails to improve for five consecutive epochs.; A meta-classifier is trained using meta-features extracted from the classification environment.  The meta-classifier then predicts the competence of base classifiers for new instances.; A meta-classifier is trained using meta-features extracted from the training data to predict the competence of base classifiers for classifying an input instance.,"Five distinct sets of meta-features are proposed, each representing a different criterion for measuring classifier competence (e.g., local accuracy, classifier confidence).  Specific details on the composition of these meta-feature sets are not provided in this section.; Five meta-features are used: 1. Local accuracy in the region of competence (f1), 2. Extent of consensus in the region of competence (f2), 3. Overall accuracy in the region of competence (f3), 4. Accuracy in the decision space (f4), 5. Classifier's confidence (f5).; Five sets of meta-features are proposed, each corresponding to a different dynamic selection criterion. Examples mentioned include classifier accuracy in the feature space and consensus in the decision space.",A meta-classifier is used to dynamically select base classifiers for an ensemble based on the extracted meta-features for a given query instance.; Dynamic ensemble selection; The meta-classifier predicts the competence of each base classifier for a given instance.  Classifiers deemed competent are then used in the ensemble.; The trained MLP acts as a meta-classifier (selector) to determine the competence of base classifiers for a given query sample.  Classifiers deemed competent by the meta-classifier are included in the ensemble.,"A pool of 100 Perceptrons generated using the bagging technique for two-class problems, and 100 multi-class perceptrons for multi-class problems.; A pool of 100 Perceptrons is used as base classifiers.; A pool of classifiers is generated (specific algorithms not mentioned in this section), and the meta-learner selects a subset for each test instance.",Accuracy; Classification accuracy is mentioned as a comparison metric.; Recognition rate (accuracy),"30 classification datasets from five repositories: UCI, KEEL, STATLOG, LKC, and ELENA.  Specific datasets mentioned include Pima, Liver Disorders, Breast (WDBC), Blood Transfusion, Banana, Vehicle, Lithuanian classes, Sonar, Ionosphere, Wine, Haberman, Cardiotocography (CTG), Vertebral Column, Steel Plate Faults, WDG V1, Ecoli, Glass, ILPD, Adult, Weaning, Laryngeal1, Laryngeal3, Thyroid, German Credit, Heart, Satimage, Phoneme, Monk2, Mammographic, and MAGIC Gamma Telescope.; 30 datasets, including 16 from the UCI machine learning repository, 4 from the STATLOG project, 4 from the KEEL repository, 4 from the Ludmila Kuncheva Collection, and 2 artificial datasets generated with Matlab PRTOOLS.; Mentioned that experiments are conducted over several small sample size classification problems, but specific datasets are not named in this section.","Experimental results are mentioned to show improvement over state-of-the-art dynamic ensemble selection techniques, but specific performance numbers are not given in this section.; The proposed META-DES achieved the highest classification accuracy in the majority of the 30 datasets compared to other dynamic selection techniques and static combination methods. Significant improvements were observed for datasets with smaller training sizes.; The proposed META-DES approach achieved results superior or equivalent to state-of-the-art DES techniques in 84% of the datasets and the highest recognition performance in 60% of the datasets.","META-DES: A dynamic ensemble selection framework using meta-learning; Proposes a novel dynamic ensemble selection framework using meta-learning with multiple criteria (meta-features) to measure base classifier competence. Aims to address limitations of using single criteria and improve robustness.; The use of meta-features leads to a more robust dynamic ensemble selection technique. The proposed META-DES framework outperforms existing DES techniques and static ensemble methods, especially on datasets with limited training data.; Using multiple DES criteria as meta-features can lead to a more robust dynamic selection technique. The proposed framework outperforms current DES techniques for many of the datasets tested.","Described in detail for each of the five meta-features (f1-f5). Involves calculations based on local accuracy, consensus, output profiles, distance to decision boundary, etc.  The process results in a meta-feature vector for each classifier and each training sample.; Meta-features are extracted from the training data in the meta-training phase.  Each training sample generates multiple meta-feature vectors, one for each classifier in the pool.  The process for generating meta-features from test data is also described, using the test data as a reference.; Meta-features are extracted from the training data, but the specific generation process for each set of meta-features is not detailed in this section.",The proposed META-DES framework performed statistically inferior to other DES techniques on a small subset (16%) of the datasets.,Not explicitly mentioned,"Rafael M.O. Cruz, Robert Sabourin, George D.C. Cavalcanti, et al., ""META-DES: A dynamic ensemble selection framework using meta-learning,"" in Meta-Learning Research, 2014."
fc5be0b604eaebaa00818259d66b3f0b.pdf,success,A New Data Characterization for Selecting Clustering Algorithms Using Meta-Learning,"Bruno Almeida Pimentel, Andr´e C.P.L.F. de Carvalho",2018,Meta-learning with ranking recommendation by Average Ranking using k-Nearest Neighbors (k-NN),"19 meta-features are used. Some are based on statistical measures (8 from [8]), some from [34] (5), and some are based on internal evaluation measures (6).  The current work proposes a new set of meta-features combining correlation and dissimilarity measures, calculated using Spearman's rank correlation coefficient and Euclidean distance respectively.; Statistical measures (LgE, LgREA, PMV, MN, SK, PO), distance-based meta-features (MF1-MF19, based on Euclidean distance and histograms), and potentially others (not fully described in this chunk)","Ranking learning techniques are used to induce a meta-model, which predicts the ranking of clustering algorithms for new datasets. The ranking of clustering algorithms is based on the quality of partitions created, assessed using internal indices.; Ranking of clustering algorithms based on predicted performance using the meta-model; Recommender system based on meta-learning using a ranking approach.  The system recommends a ranking of clustering algorithms.",10 clustering algorithms are considered (not explicitly named in this section); 7 clustering algorithms (not explicitly named in this chunk),"SRC, ARI, AMI, Spearman's coefficient; Spearman's rank correlation coefficient (SRC), Adjusted Rand's Index (ARI), and Adjusted Mutual Information (AMI)","219 datasets from OpenML covering various domains (e.g., engineering, biology, medicine). Four meta-datasets are created based on different sets of meta-features: Statistical, Distance, Evaluation (from previous works), and CaD (the proposed approach).; 32 microarray gene expression datasets (from Souto et al. (2008)), 84 datasets from the UCI repository (from Ferrari and de Castro (2015)), 22 synthetic datasets and 14 datasets from the UCI repository (from Adam and Blockeel (2015)), and another set of 32 microarray gene expression datasets (from Vukicevic et al. (2016)). This chunk mentions experiments with a wider variety of datasets.","At the meta-level, the rankings recommended by the proposed approach were statistically significantly better than those recommended by meta-features from the related literature and closer to the true rankings (using Spearman’s coefficient) with K-Nearest Neighbor. At the base level, for both K-NN and Random Forest meta-models, the proposed method's top recommended clustering algorithm achieved better clustering quality (ARI and AMI).; Not explicitly mentioned in this section","A new set of meta-features combining correlation and dissimilarity measures improved clustering algorithm recommendation. The proposed approach outperformed existing meta-learning based recommenders.  Analysis of meta-feature importance revealed some meta-features are more influential in recommendations. RF was more robust than k-NN with increasing hyperparameter values.; Proposes a new approach using a new set of meta-features combining correlation and dissimilarity measures. Aims to address deficiencies of previous work, such as the low number of datasets, limited set of clustering algorithms, and lack of exploration of result interpretation for recommender systems.; Proposes a new set of meta-features based on correlation and dissimilarity measures (not detailed in this chunk). Evaluates the importance of each meta-feature for clustering algorithm recommendation.","Combines correlation and dissimilarity measures (details not provided in this chunk); Described for statistical and distance-based meta-features.  The new proposed meta-features generation process is not described in this chunk.; Described in Algorithm 1 and involves calculating correlation (Spearman's rank) and dissimilarity (Euclidean distance) between instances, concatenating and normalizing the resulting vectors, and then calculating 19 meta-features based on the normalized vector.","Could benefit from new meta-features capturing other dataset aspects, more datasets for increased meta-learner reliability, exploration of other classification algorithms in the recommender, and attribute selection for extracted meta-features.; Previous studies had limitations such as a low number of datasets, a limited set of clustering algorithms, and insufficient use of internal validation measures. This work aims to address these limitations.","This paper introduces a novel meta-learning approach for clustering algorithm selection using a new set of meta-features based on correlation and dissimilarity measures.  Employing a ranking recommendation method with k-Nearest Neighbors and Random Forest, the proposed approach outperformed existing methods by recommending clustering algorithms that achieved higher Adjusted Rand Index and Adjusted Mutual Information scores on a diverse collection of datasets.  The findings demonstrate the effectiveness of the new meta-features in characterizing datasets for improved clustering algorithm recommendation.","Bruno Almeida Pimentel, Andr´e C.P.L.F. de Carvalho, ""A New Data Characterization for Selecting Clustering Algorithms Using Meta-Learning,"" in Meta-Learning Research, 2018."
