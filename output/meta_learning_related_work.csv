filename,extraction_status,Paper Title,Authors,Year of Publication,Meta-Learning Method Used,Meta-Features Chosen,Algorithm Selection Method,Algorithms Considered for Selection,Evaluation Metrics,Dataset(s) Used,Performance of Meta-Learning Approach,Key Findings/Contributions,Meta-Feature Generation Process,Limitations,Simple Summary,IEEE Citation
2205.10362v2.pdf,success,FIND: Explainable Framework for Meta-learning,"Xinyue Shao, Hongzhi Wang, Xiao Zhu, and Feng Xiong",2018,"Neural network with one input layer, two hidden layers, and one output layer","23 meta-features are used, including target attribute categories, number of category attributes, number of numeral attributes, information entropy of attribute values within each category, and others.; Three types: simple (size, number of attributes, number of categories), statistical (geometric mean, variance), and information-theoretic (information entropy)","A meta-learning approach is used to recommend algorithms.  Details of the specific algorithm used for meta-learning (e.g., neural network architecture) are not provided in this chunk, but it involves a training process with dropout and learning rate settings.; Based on learned mapping between meta-features and algorithm performance using the neural network. Integrated gradients are used to explain the meta-learning process.; Classification algorithms (mentioned as having a wide selection) and potentially regression algorithms (referenced from previous work); Model recommendation","Not explicitly mentioned in this chunk, but referred to as S1, S2,...Sm; adaboostM1, Random Forest, XGBoost, MLP, ClassificationViaRegression are mentioned as examples.","Accuracy, Precision, Recall, F-measure, F1, F2, F3; Not explicitly mentioned in this chunk; Reliability and efficiency of counterfactual generation, average distance of counterfactuals from original instances, average time cost for counterfactual generation","A custom metadata set created for this research, including datasets like anneal, D41, D74, heart-statlog, liver-disorders, monks-problems-1, sick.  Adult and German-Credit datasets are used for evaluating feature influence computation.; Adult, German-Credit; Common machine learning datasets and the best performing models on each dataset are used as the training set for the meta-learning process.","Achieved over 80% accuracy in recommending algorithms, outperforming Random Forest and XGBoost. The recommended methods also showed comparable performance to the labeled best methods, even when the recommendation was different.; Not explicitly mentioned in this chunk","(1) A comprehensive framework for explainable meta-learning called FIND. (2) An interpretable learning algorithm recommendation approach based on dataset meta-features. (3) A novel feature importance metric based on causality and a greedy counterfactual generation approach. (4) Evaluation of FIND's accuracy and validity on real datasets.; Explainable meta-learning approach for algorithm selection using integrated gradients. Proposes a feature influence computation method incorporating causality and latent factors.; Proposed and implemented the FIND meta-learning explainability framework which uses model recommendation to achieve meta-explainability. Recommendation explainability is achieved through feature influence computation and counterfactual generation.; The proposed meta-learning module can effectively recommend efficient decision methods. The integrated gradients analysis provides insights into the relationship between dataset characteristics and suitable algorithms (e.g., adaboostM1 for multi-class tasks, Random Forest for datasets with many categorical features).  The feature influence computation method incorporates causality and performs comparably to LIME and SHAP.","Data is collected on characteristics like target attribute categories, number of category/numeral attributes, and information entropy.; Extracted from datasets using statistical algorithms. The process retrieves dataset statistics as meta-features.","Explainability in more challenging meta-learning domains (e.g., time series) and deeper counterfactual exploration incorporating causality need further investigation.; Not explicitly mentioned in this chunk","This paper introduces FIND, a framework for explainable meta-learning that recommends machine learning algorithms based on dataset meta-features.  Using a neural network trained on a custom metadata set, FIND achieves over 80% accuracy in algorithm recommendation and provides explanations for these recommendations via integrated gradients and a novel causality-based feature importance metric.  The framework demonstrates the effectiveness of using meta-features for algorithm selection and offers insights into the relationship between dataset characteristics and algorithm performance.","Xinyue Shao, Hongzhi Wang, Xiao Zhu, et al., ""FIND: Explainable Framework for Meta-learning,"" in Meta-Learning Research, 2018."
2410.07696v1.pdf,success,Meta-Learning from Learning Curves for Budget-Limited Algorithm Selection,"Manh Hung Nguyen, Lisheng Sun-Hosoya, and Isabelle Guyon",2024,"Reinforcement learning (specifically, Double Deep Q-Network (DDQN) and other unspecified methods used by participants)","Dataset meta-features and algorithm hyperparameters were provided, but specific features chosen by participants are not mentioned.; Learning curves, dataset meta-features, and less frequently algorithm hyperparameters","Learned policies (often combined with hard-coded rules) based on meta-learned information such as algorithm rankings and learning curve progression; Markov Decision Process (MDP) framework where an agent learns a selection policy to choose algorithms based on partial learning curves, meta-features, and hyperparameters.; Reinforcement Learning (specifically, agents were submitted to a challenge)","A set of 40 algorithms with varying hyperparameters (details not provided); Round 1: Tree-based algorithms (Random Forest, Gradient Boosting) with varying hyperparameters. Round 2: Nearest Neighbors, Multilayer Perceptron, Adaboost, and Stochastic Gradient Descent with varying training data size.; SGD, AdaBoost, KNN","Area under the learning curve (ALC) of the agent's performance on test sets, accumulated reward during each episode.; Average accumulated reward (ALC) over meta-test datasets","30 cross-domain AutoML datasets provided by [9], along with a synthetic meta-dataset created by [17, 18] for practice.; Flora, waldo, pablo, marco, evita, wallis, jannis, dionis, alexis, cadata, carlo, digits, dorothea, fabert (some datasets where BoS beat DDQN); Meta-datasets consisting of pre-recorded learning curves from the AutoML challenge, including datasets like tania, robert, newsgroups, and marco.  Split into meta-train and meta-test sets.","Mentioned that meta-learning enhances algorithm selection, evidenced by winning teams and a DDQN baseline, outperforming heuristic baselines and random search. A cost-effective baseline performs decently when learning curves don't intersect frequently.; Top-performing teams outperformed the DDQN baseline in some rounds and settings. The use of meta-learning and learning curve progression was shown to significantly improve performance.; Winning agents using meta-learned knowledge outperformed those without, particularly through targeted exploration of algorithm choices.  A simple Best on Samples (BoS) baseline performed decently and even beat the DDQN baseline on some datasets (9/30 in the first round and 5/15 in the second round) where algorithm learning curves didn't intersect often.","(i) Formulation of the limited-budget algorithm selection problem as an MDP. (ii) Introduction of benchmark datasets of learning curves. (iii) Comprehensive result analysis including comparison of data usage and policy types, ablation study showing benefits of meta-learning and learning from learning curves, and examination of strategies learned by winning methods.; Agents leveraging meta-learned knowledge of learning curve progressions perform better. A simple BoS method can be a competitive baseline. The challenge datasets serve as a benchmark for future research.; Learning curves and dataset meta-features were important meta-features. Switching explored algorithms frequently is not beneficial.  Starting with a small budget to get a ""base score"" and avoiding small budget allocations near the end of an episode are beneficial strategies.",Learning curves were pre-recorded from the AutoML challenge. Dataset meta-features and algorithm hyperparameters were also used (generation process not detailed).; Meta-features and hyperparameters were provided as part of the datasets; the generation process is not detailed.,Potential overfitting on test learning curves due to high correlation with validation curves in the first round of the challenge. The pre-recorded nature of the learning curves might not fully reflect real-world scenarios.,"This paper introduces a meta-learning approach for budget-limited algorithm selection, framing the problem as a Markov Decision Process (MDP) where reinforcement learning agents learn policies to select algorithms based on partial learning curves, dataset meta-features, and hyperparameters.  Evaluation on a benchmark of pre-recorded learning curves demonstrated that meta-learning significantly improves algorithm selection performance, with winning agents leveraging learning curve progression and dataset meta-features to outperform heuristic baselines.  Key findings highlight the importance of meta-learned knowledge for targeted algorithm exploration and the surprising competitiveness of a simple Best on Samples baseline.","Manh Hung Nguyen, Lisheng Sun-Hosoya, and Isabelle Guyon, ""Meta-Learning from Learning Curves for Budget-Limited Algorithm Selection,"" in Meta-Learning Research, 2024."
A_Literature_Survey_and_Empirical_Study_of_Meta-Learning_for_Classifier_Selection.pdf,success,A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection,"Irfan Khan, Xianchao Zhang, Mobashar Rehman, and Rahman Ali",2020,Not explicitly mentioned,"2) Problem-Complexity-based measures; 3) Model-Structure-based measures; 4) Landmarking-based measures; Discusses various groups of meta-features used for classifier selection, categorized into: 1) Simple, Statistical, and Information-Theory-based measures; Statistical, structural, and information-based meta-features, including frequencies of itemsets with respect to the parity function.  Specifically, one-itemsets (VI) capturing attribute value distributions and two-itemsets (VII) representing feature correlations are used. Statistical summaries (min, max, and seven octiles) of these itemsets are computed.; and 5) Structural-Information-based measures (though the description for this last group is cut off).  Specific examples of measures within these groups are mentioned, such as Maximum Fisher's discriminant ratio and Entropy of Class Proportions.","Generic framework described involving meta-knowledge construction (performance evaluation and dataset characterization), mapping model construction (transforming meta-data into a learning dataset, applying MCP, and training a learning algorithm), and recommendation for new instances.; Various meta-learners are discussed, including decision trees (C4.5, C5.0), k-Nearest Neighbors (KNN), regression algorithms, multi-label learning (ML-KNN), clustering, and link prediction (LRW, SRW). The choice of meta-learner depends on the meta-target.","Candidate algorithms (algorithm space A) are selected to potentially solve problem instances. Specific algorithms are not named in this section.; Classification algorithms (17 mentioned in later sections); The text mentions 22 algorithms used with C4.5, 8 with C5.0, 10 with KNN in the DMA system, 13 in a multi-label learning approach, and 21 in a link prediction approach. Specific algorithm names are not provided.","Accuracy and runtime are mentioned as performance evaluation measures.; Adjusted Ratio of Ratios (ARR) is mentioned as a multi-criteria performance evaluation metric at the meta-level.; Performance metrics (e.g., accuracy, runtime - mentioned in later sections)","84 benchmark datasets (mentioned in later sections); A subset of diverse classification problems (problem space P) are selected for building the meta-knowledge database. Specific datasets are not named in this section.; The text mentions the use of datasets from the UCI Machine Learning Repository. Specific datasets and the number used in different studies are mentioned (e.g., 67 datasets with 10 algorithms, 84 datasets with 13 algorithms, 131 datasets with 21 algorithms), but no dataset names are given.","Not explicitly mentioned in this section, although it discusses the advantages and disadvantages of different meta-learning approaches.; Not explicitly mentioned in this section. The text focuses on the framework and components of meta-learning for classifier selection.","First survey and comparative study on meta-learning for classification algorithm selection. Presents a generic workflow for meta-learning based algorithm recommendation.  Critically analyzes existing studies based on meta-features, meta-learner, and meta-target. Performs extensive comparative evaluation of prominent classifier selection methods.; This paper presents the first literature survey and extensive empirical study specifically focused on meta-learning based classifier selection. It also performs a detailed comparative evaluation study due to the lack of such studies in prior work.; This section provides a summary of different meta-learning approaches for algorithm selection, including the types of meta-learners, meta-features, and meta-targets used. It also discusses the advantages and disadvantages of each approach.","Described for the provided example measures (Maximum Fisher's discriminant ratio and Entropy of Class Proportions).  General processes for each meta-feature group are also discussed.; The process involves converting datasets to binary format, generating one-itemsets (VI) and two-itemsets (VII), sorting them, and then calculating statistical summaries (min, max, and seven octiles).","Development of domain-specific meta-features is considered difficult and an impediment in applying meta-learning to new domains.  Prior works often only compare newly proposed methods with one or two other methods, use the same group of meta-features without necessarily using the same extraction measures, and employ diverse candidate algorithms and datasets, making quantitative assessment difficult.; Existing literature lacks a comprehensive survey and comparative study of meta-learning methods for classifier selection.  Each new method is typically compared with only one or two previous methods.; The text discusses limitations related to choosing the optimal value for K in KNN, the computational cost of retraining regression models, the selection of an appropriate number of datasets for the meta-level, and potential bias introduced by dataset selection.",Not explicitly mentioned,"Irfan Khan, Xianchao Zhang, Mobashar Rehman, et al., ""A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection,"" in Meta-Learning Research, 2020."
