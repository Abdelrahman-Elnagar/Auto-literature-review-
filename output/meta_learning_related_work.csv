filename,extraction_status,Paper Title,Authors,Year of Publication,Meta-Learning Method Used,Meta-Features Chosen,Algorithm Selection Method,Algorithms Considered for Selection,Evaluation Metrics,Dataset(s) Used,Performance of Meta-Learning Approach,Key Findings/Contributions,Meta-Feature Generation Process,Limitations,Simple Summary,IEEE Citation,error_message
1-s2.0-S0020025522000639-main.pdf,success,Auto-CASH: A meta-learning embedding approach for autonomous classification algorithm selection,"Tianyu Mu, Hongzhi Wang, Chunnan Wang, Zheng Liang, Xinyue Shao",2022,"Not explicitly mentioned; Auto-CASH, which utilizes Deep Q-Network (DQN) for automatic meta-feature selection and Random Forest (RF) as the meta-learner for algorithm selection.; Auto-CASH, a meta-learning based approach for the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem. It represents the task as a feature vector and models the algorithm selection experience as the mapping from analysis task to its optimal process algorithm. A meta-learner is trained with the knowledge to identify high-performing algorithms for solving new tasks.; Random Forest (RF) model serves as the surrogate model of the meta-learner, which is used to recommend algorithms for new tasks based on extracted meta-feature vectors.; Deep Q-Network (DQN) for automatic meta-feature selection and Genetic Algorithm (GA) for hyperparameter optimization (HPO).; Auto-CASH employs a model-free reinforcement learning strategy for feature selection and algorithm recommendation, leveraging historical experience to train a machine learning agent model.; Auto-CASH, which combines meta-learning and reinforcement learning (Deep Q-Network) for the CASH (Combined Algorithm Selection and Hyperparameter optimization) problem.","Not explicitly mentioned; Meta-features are automatically selected using a reinforcement learning strategy (DQN), reducing dependency on human expertise.; Meta-features are automatically selected and transformed into meta-data using a reinforcement policy and a trained network. This process reduces human intervention and considers complex correlations among features.; Meta-features are automatically determined through Deep Q-Network (DQN). Each task is described with a meta-feature vector, which includes features such as class entropy and variance.; A set of 23 statistical meta-features capturing various aspects of datasets, such as class information entropy, proportion of numeric attributes, and variance in numeric attributes.; The meta-features are selected using Deep Q-Networks (DQN) to represent dataset characteristics comprehensively, reducing the number of meta-features to 7 while avoiding overlapping information.; The meta-features are classified into five categories. Auto-Model uses a list of meta-features (MAM list) spread across these categories, while DQN's selection is more concentrated in the second and fifth categories.","Not explicitly mentioned; Random Forest (RF) is used to rank algorithms and select the best one for the input task based on the meta-features selected by DQN.; Auto-CASH automatically selects the best algorithm for the uploaded task and performs hyperparameter optimization to provide a configured model. It uses a genetic search with two prune strategies for hyperparameter configuration.; Algorithm selection is based on meta-learning theory, where a Random Forest model trained on meta-data (meta-feature vectors and their optimal algorithms) recommends an algorithm for a new task.; DQN is used to automatically select optimal meta-features, which are then used to train a meta-model for algorithm selection. GA is used for hyperparameter tuning of selected algorithms.; Auto-CASH uses meta-learning to accelerate algorithm selection, combining empirical learning with genetic search pruning strategies to approach optimal performance without iterative search.; Deep Q-Network (DQN) is used to transform the selection of meta-features into a continuous action-selection problem, enabling automatic meta-feature selection.",Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,,"Tianyu Mu, Hongzhi Wang, Chunnan Wang et al., ""Auto-CASH: A meta-learning embedding approach for autonomous classification algorithm selection"", in Proc. Meta-Learning Conf., 2022.",
1-s2.0-S0377221721003623-main.pdf,success,Machine learning at the service of meta-heuristics for solving combinatorial optimization problems: A state-of-the-art,"Maryam Karimi-Mamaghan, Mehrdad Mohammadi, Patrick Meyer, Amir Mohammad Karimi-Mamaghan, El-Ghazali Talbi",2021,"Not explicitly mentioned; Integration of machine learning techniques into meta-heuristics for solving combinatorial optimization problems.; Not explicitly mentioned in the provided chunk.; Meta-learning is employed to learn the problem-algorithm mapping on a set of training instances and create a meta-model, which is then used to predict the appropriate mapping for new problem instances.; The paper discusses Algorithm Selection Problem (ASP) and its application in selecting the most appropriate metaheuristic (MH) for solving combinatorial optimization problems (COPs). It emphasizes the use of meta-models created from training instances to guide algorithm selection.; The paper discusses the use of fitness approximation and fitness reduction techniques in metaheuristic algorithms (MHs) to reduce computational effort in evaluating fitness functions, particularly in computationally expensive scenarios.; The paper discusses the integration of Machine Learning (ML) techniques into Meta-Heuristics (MHs) for generating initial solutions, improving convergence rates, and enhancing the exploration-exploitation balance. Techniques such as k-means clustering, Q-Learning (QL), Reinforcement Learning (RL), and Opposition-based Learning are highlighted.; Reinforcement Learning (RL)-based methods such as Simple Credit Assignment (SCA), Q-Learning Credit Assignment (QLCA), and Learning Automata Credit Assignment (LACA) are used for credit assignment in operator selection.; The paper discusses the use of Adaptive Operator Selection (AOS) methods, including PMS, APS, SMS, UCB-MABS, EGS, and HS, to balance exploration and exploitation in metaheuristics (MHs). It also introduces Learnable Evolution Models (LEMs) that integrate machine learning (ML) techniques to generate new populations based on learned hypotheses.; Learning Evolution Model (LEM) with two versions: uniLEM (solely learning mode) and duoLEM (coupled Darwinian and learning evolution processes).; Not explicitly mentioned in the provided chunk.; The paper discusses the integration of Machine Learning (ML) techniques into Metaheuristics (MHs) for solving Combinatorial Optimization Problems (COPs). Specific ML techniques mentioned include Reinforcement Learning (RL), Apriori algorithms, and k-means clustering.; Few-Shot Learning is suggested to train models with minimal data by leveraging prior knowledge of similar problem instances.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Coalition-based metaheuristic using reinforcement learning and mimetism; Not explicitly mentioned in the provided chunk.","Not explicitly mentioned; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Meta-features are a set of quantitative features that represent the properties of a problem instance. Examples include features for SAT, TSP, AP, OP, KP, BPP, and GCP.; Meta-features include basic descriptive statistics (e.g., minimum, maximum, mean, median) and more complex features such as landscape features of COPs (e.g., edge and vertex measures, complex network measures like average geodesic distance, network vulnerability, and target entropy).; The meta-features include the characteristics of the optimization problem (single/multi-objective), the type of fitness evaluation approach (approximation/reduction), and the computational complexity of the fitness function.; The meta-features include the performance criteria such as Objective Function Value (OFV), Diversity of Solutions (DOS), Computational Time (CT), and Depth of Local Optima (DLP).; Performance criteria such as Objective Function Value (OFV), Degree of Solution (DOS), and Computational Time (CT) are used as meta-features.; The meta-features are not explicitly mentioned in the provided chunk. However, the states in the QLCA method are defined as search-dependent, problem-dependent, or instance-dependent, which could be considered as meta-features.; Qualitative descriptions that discriminate between H-group (high fitness) and L-group (low fitness) individuals in the population.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.","Not explicitly mentioned; Review and classification of recent advances in using machine learning techniques for algorithm selection, fitness evaluation, initialization, evolution, parameter setting, and cooperation.; ML techniques are used to predict the performance of Metaheuristics (MHs) in solving optimization problems, aiding in the selection of appropriate MHs.; Algorithm Selection Problem (ASP) is used to automatically select the most appropriate algorithm(s) for solving a problem instance using meta-learning techniques.; The paper discusses both single algorithm selection (SLC) and multiple algorithm selection (MLC and RLC). It also mentions the potential for online ASP, where the algorithm selection mechanism adapts during the resolution of problem instances.; The selection of fitness approximation techniques depends on the combinatorial optimization problem (COP) under study and the user's preferences. The simplest techniques are tried first, and more sophisticated methods are used if performance is unsatisfactory.; Adaptive Operator Selection (AOS) is used to dynamically select and apply the most appropriate operator during the search process based on historical performance.; Various selection methods are used, including Random Selection (RS), Maximum Credit Selection (MCS), Roulette Wheel Selection (RWS), Proportional Minimum Selection (PMS), Adaptive Proportional Selection (APS), Simulated Annealing Selection (SMS), Upper Confidence Bound Multi-Armed Bandit Strategy (UCB-MABS), Dynamic Multi-Armed Bandit Strategy (D-MABS), Epsilon-Greedy Selection (EGS), and Heuristic Selection (HS).; The paper describes various AOS methods (PMS, APS, SMS, UCB-MABS, EGS, HS) and the QLCA method for operator selection. LEMs use ML techniques for hypothesis generation and instantiation to guide the evolution process.; Rule learning techniques such as AQ learners, decision tree learners (e.g., C4.5), and ANN for hypothesis generation.; Parameter tuning and parameter control are discussed, with methods including brute force experiments, Design of Experiments (DOE), racing procedures, meta-optimization, and machine learning techniques like LogR, LR, SVM, and ANN.; The paper suggests using ML techniques to adapt the behavior of cooperative MHs based on the characteristics of the search space. This includes both inter-agent and intra-agent learning levels.; The algorithm selection problem is highlighted as a research direction to choose the most appropriate ML technique for integration into Metaheuristics (MHs).; Reinforcement learning and meta-learning based approaches are mentioned for dynamic algorithm selection and adaptive parameter control.; Not explicitly mentioned in the provided chunk.; Automatic algorithm selection approach; Not explicitly mentioned in the provided chunk.",Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,"This paper provides a comprehensive review of how machine learning (ML) techniques are integrated into meta-heuristics (MHs) to enhance the solving of combinatorial optimization problems (COPs). It highlights the use of ML for algorithm selection, fitness approximation, initialization, evolution, and adaptive operator selection, emphasizing methods like reinforcement learning, k-means clustering, and Learnable Evolution Models (LEMs). The study also discusses the Algorithm Selection Problem (ASP) and the creation of meta-models to predict the most effective MHs for specific problem instances, offering a roadmap for future research in this interdisciplinary field.","Maryam Karimi-Mamaghan, Mehrdad Mohammadi, Patrick Meyer et al., ""Machine learning at the service of meta-heuristics for solving combinatorial optimization problems: A state-of-the-art"", in Proc. Meta-Learning Conf., 2021.",
1-s2.0-S092523122200474X-main.pdf,success,A comprehensive survey on recent metaheuristics for feature selection,"Tansel Dokeroglu, Ayça Deniz, Hakan Ezgi Kiziloz",2022,"Not explicitly mentioned; Survey of recent metaheuristic algorithms for feature selection; The paper reviews various metaheuristic algorithms for feature selection, including modified versions of Particle Swarm Optimization (PSO), Genetic Algorithms (GA), and Differential Evolution (DE).; Not explicitly mentioned in the provided chunk.; The paper discusses various metaheuristic algorithms inspired by biological behaviors, including Bacterial Foraging Optimization (BFO), Bat Algorithm (BA), Biogeography-Based Optimization (BBO), Butterfly Optimization Algorithm (BOA), Crow Search Algorithm (CSA), Cuckoo Search (CS), and Dragonfly Algorithm (DA).; The paper discusses various meta-heuristic algorithms inspired by nature, including Dragonfly Algorithm (DA), Firefly Algorithm (FA), Grasshopper Optimization Algorithm (GOA), Gravitational Search Algorithm (GSA), and Grey Wolf Optimization (GWO). These algorithms are used for feature selection and optimization tasks.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the chunk.; Not explicitly mentioned in the provided chunk.; Hybrid metaheuristic algorithms combining various optimization techniques for feature selection.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; The paper discusses various meta-heuristic algorithms for feature selection, including Harris’ Hawks Optimization, Krill Herd Algorithm, Salp Swarm Algorithm, Sine Cosine Optimization, Social Spider Algorithm, Teaching-Learning-Based Optimization, Whale Optimization Algorithm, and others.; Not explicitly mentioned in the provided chunk.","Not explicitly mentioned; Exploration/exploitation operators, selection methods, transfer functions, fitness value evaluations, and parameter setting techniques; The meta-features include binary encoding of selected feature subsets, fitness functions that balance classification error and feature size, and multiobjective optimization criteria.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; The meta-features are not explicitly mentioned in the provided chunk. However, the algorithms are designed to optimize feature selection, which implies that the features of the datasets (e.g., number of attributes, class distribution) are considered indirectly.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.","Not explicitly mentioned; Focus on recent metaheuristics with promising results, computation performance, prediction accuracy, and main contributions; The authors compare different metaheuristic algorithms based on their performance in feature selection tasks, using fitness functions and multiobjective optimization criteria.; Not explicitly mentioned in the provided chunk.; The algorithms are selected based on their biological inspiration and their application to optimization problems, particularly feature selection.; The algorithms are selected based on their ability to optimize feature selection and solve combinatorial problems. The selection is driven by the need to balance exploration and exploitation in the search space.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the chunk.; The selection of algorithms is based on their nature, inspiration, and leadership mechanisms, such as global best, local best, or random walks.; Selection of algorithms based on their ability to balance exploration and exploitation, and their performance in feature selection tasks.; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; The paper employs a variety of meta-heuristic algorithms for feature selection, often hybridized with other optimization techniques to improve performance.; Not explicitly mentioned in the provided chunk.",Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,"This paper provides a comprehensive survey of recent metaheuristic algorithms for feature selection, including modified versions of Particle Swarm Optimization (PSO), Genetic Algorithms (GA), and Differential Evolution (DE), as well as nature-inspired algorithms like Firefly Algorithm (FA) and Grey Wolf Optimization (GWO). The authors evaluate these algorithms based on their ability to balance exploration and exploitation, optimize feature selection, and improve computational performance and prediction accuracy. The survey highlights the strengths and applications of various metaheuristics, offering insights into their effectiveness in addressing feature selection challenges.","Tansel Dokeroglu, Ayça Deniz, Hakan Ezgi Kiziloz, ""A comprehensive survey on recent metaheuristics for feature selection"", in Proc. Meta-Learning Conf., 2022.",
2107.09414v1.pdf,success,Algorithm Selection on a Meta Level,"Alexander Tornede, Lukas Gehring, Tanja Tornede, Marcel Wever, Eyke Hüllermeier",2021,"Not explicitly mentioned; The paper introduces meta algorithm selection, which combines existing algorithm selection methods into a single superior algorithm selector using meta learning and ensemble learning techniques.; Meta learning is used to select the best algorithm selector for a given instance, effectively treating algorithm selectors as algorithms in a standard algorithm selection problem.; The paper discusses various meta-learning methods for algorithm selection, including voting, bagging, boosting, and stacking. Voting ensembles train multiple algorithm selectors independently and aggregate their predictions. Bagging uses bootstrapping to create diverse training sets for a single type of algorithm selector. Boosting re-weights training instances iteratively to focus on misclassified instances. Stacking learns a meta-learner to aggregate predictions from base algorithm selectors.; The paper discusses various meta-learning methods including voting ensembles, bagging ensembles, boosting ensembles, and stacking. These methods are used to improve the performance of algorithm selectors by combining multiple base algorithm selectors.; The meta-learning approach involves learning a mapping from instances to a set of algorithm selectors and an aggregation function. The approach uses ensemble methods such as voting, bagging, stacking, and boosting to aggregate the predictions of multiple selectors.; Not explicitly mentioned in the provided chunk.","Not explicitly mentioned; Not explicitly mentioned in the provided chunk.; Not explicitly mentioned in the provided chunk.; The meta-features are not explicitly mentioned in the provided chunk, but the stacking method uses an extended feature representation that includes predictions from base algorithm selectors.; The paper does not explicitly mention the specific meta-features chosen. However, it discusses the use of additional features generated by base algorithm selectors in stacking ensembles.; The meta-features include instance features and predictions from base algorithm selectors. The feature importance is analyzed using a multi-class classification meta-learner with a one-vs-all decomposition and a random forest classifier.; Not explicitly mentioned in the provided chunk.","Not explicitly mentioned; The paper proposes a general methodological framework for meta algorithm selection, including several concrete learning methods that combine ideas of meta learning and ensemble learning.; The method involves selecting one or multiple algorithm selectors and aggregating their decisions to return a single algorithm. This is formalized as finding an algorithm selector selector (ASS) and an aggregation function.; The algorithm selection is performed using ensemble methods such as voting, bagging, boosting, and stacking. The Borda count is used as an aggregation function for ranking algorithms.; The paper evaluates different algorithm selection methods, including voting, bagging, boosting, and stacking, to improve the performance of base algorithm selectors.; Algorithm selection is performed using ensemble methods that aggregate the predictions of multiple base algorithm selectors. The methods include voting, bagging, stacking, and boosting with various aggregation functions such as weighted majority and Borda count.; Algorithm selection is discussed in the context of dyadic feature representation, collaborative filtering, and survival analysis.",Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,"The paper ""Algorithm Selection on a Meta Level"" introduces a novel framework for meta algorithm selection, which combines existing algorithm selection methods using meta-learning and ensemble techniques to create a superior algorithm selector. It explores various ensemble methods, including voting, bagging, boosting, and stacking, to aggregate predictions from multiple base algorithm selectors and improve overall performance. The key contribution lies in formalizing the problem as selecting an ""algorithm selector selector"" (ASS) and proposing a methodological approach to enhance algorithm selection through meta-learning.","Alexander Tornede, Lukas Gehring, Tanja Tornede et al., ""Algorithm Selection on a Meta Level"", in Proc. Meta-Learning Conf., 2021.",
2205.10362v2.pdf,error,FIND: Explainable Framework for Meta-learning,"Xinyue Shao, Hongzhi Wang, Xiao Zhu, Feng Xiong",2018,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a66b90 state=finished raised KeyError>]
229655909.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a65ab0 state=finished raised KeyError>]
2305.09101v1.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a670d0 state=finished raised KeyError>]
2410.07696v1.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a66b00 state=finished raised KeyError>]
A_Literature_Survey_and_Empirical_Study_of_Meta-Learning_for_Classifier_Selection.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a67be0 state=finished raised KeyError>]
cruz2015.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a67130 state=finished raised KeyError>]
fc5be0b604eaebaa00818259d66b3f0b.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a65b10 state=finished raised KeyError>]
ferrari2015.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a67af0 state=finished raised KeyError>]
fulltext61022017.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a67a00 state=finished raised KeyError>]
Metaheuristic_Algorithms_on_Feature_Selection_A_Survey_of_One_Decade_of_Research_2009-2019.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a67850 state=finished raised KeyError>]
rossi2014.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a66440 state=finished raised KeyError>]
s10994-022-06161-4.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a674f0 state=finished raised KeyError>]
smith-miles2008.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a65990 state=finished raised KeyError>]
Wrapper_and_Hybrid_Feature_Selection_Methods_Using_Metaheuristic_Algorithms_for_English_Text_Classification_A_Systematic_Review.pdf,error,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,Not explicitly mentioned,RetryError[<Future at 0x21034a66590 state=finished raised KeyError>]
